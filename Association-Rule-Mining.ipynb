{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Association Rule Mining\n",
    "\n",
    "- **100 points [8% of your final grade]**\n",
    "- **Due Sunday, April 28 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* implement the association rule mining process with the Apriori algorithm.\n",
    "\n",
    "- *Submission instructions:* for this homework, you only need to submit to Blackboard. Please name your submission **FirstName_Lastname_hw4.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw4.ipynb**. Your notebook should be fully executed so that we can see all outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are going to examine movies using our understanding of association rules. For this part, you need to implement the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of user-movie matrix: shape (11743, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
    "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, there are two columns: the first column is the integer ids of users, and the second column is the integer ids of movies. Each row denotes that the user of given user id watched the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been watched by a single user as a transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user movie-watching behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. \n",
    "\n",
    "**Note: Do not copy-paste any existing code.**\n",
    "\n",
    "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
    "\n",
    "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will explain the process of the method.**\n",
    "\n",
    "**Hint: If you implement your algorithm correctly, you should be able to see intermediate result as:**\n",
    "- Candidates of length 1 count: 408\n",
    "- After Pruning count: 21\n",
    "- Candidates of length 2 count: 210\n",
    "- After Pruning 2 count: 36\n",
    "- Candidates of length 3 count: 55\n",
    "- After Pruning 3 count: 12\n",
    "- Candidates of length 4 count: 1\n",
    "- After Pruning 4 count: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating candidates of length 1\n",
      "Candidates of length 1 count: 408\n",
      "After Pruning count: 21\n",
      "Generating candidates of length 2\n",
      "Candidates of length 2 count: 210\n",
      "After Pruning count: 36\n",
      "Generating candidates of length 3\n",
      "Candidates of length 3 count: 55\n",
      "After Pruning count: 12\n",
      "Generating candidates of length 4\n",
      "Candidates of length 4 count: 1\n",
      "After Pruning count: 0\n",
      "No frequent itemsets of length 4. Ending the algorithm.\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "# include all the helpful print statements\n",
    "# when you run this block, we want to see all of your intermediate steps\n",
    "# you can save the rules you discover for printing in the following cells (this will help us grade by keeping these separate)\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    transactions = {}\n",
    "    for user, movie in data:\n",
    "        transactions.setdefault(user, list()).append(int(movie))\n",
    "    return list(transactions.values())\n",
    "\n",
    "def generate_candidate(data, i):\n",
    "    # Flatten the list of transactions to find all unique items\n",
    "    unique_items = set()\n",
    "    for transaction in data:\n",
    "        for item in transaction:\n",
    "            unique_items.add(item)\n",
    "\n",
    "    # Generate all combinations of the unique items of length 'i'\n",
    "    candidates = []\n",
    "    for combination in combinations(unique_items, i):\n",
    "        # Convert each combination from a tuple to a set\n",
    "        candidate = set(combination)\n",
    "        candidates.append(candidate)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def calculate_support(transactions, candidate):\n",
    "    \"\"\"\n",
    "    Calculate the support for a single candidate itemset.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for transaction in transactions: \n",
    "        is_subset = True\n",
    "        for item in candidate:\n",
    "            if item not in transaction:\n",
    "                is_subset = False\n",
    "                break\n",
    "        if is_subset:\n",
    "            count += 1  \n",
    "    support = count / len(transactions)\n",
    "    return support\n",
    "\n",
    "def filter_candidates_by_support(transactions, candidates, min_support):\n",
    "    \"\"\"\n",
    "    Filter candidate itemsets that have a support greater than or equal to the given minimum support.\n",
    "    \"\"\"\n",
    "    frequent_itemsets = []\n",
    "    for candidate in candidates:\n",
    "        support = calculate_support(transactions, candidate)\n",
    "        if support >= min_support:\n",
    "            frequent_itemsets.append(candidate)\n",
    "    return frequent_itemsets\n",
    "\n",
    "def has_infrequent_subset(candidate, all_frequent_itemsets):\n",
    "    for i in range(1, len(candidate)):\n",
    "        subsets = combinations(candidate, i)\n",
    "        for subset in subsets:\n",
    "            frozen_subset = frozenset(subset)\n",
    "            if frozen_subset not in all_frequent_itemsets:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def apriori_algorithm(data, x, min_support):\n",
    "    all_frequent_itemsets = []\n",
    "\n",
    "    # This will hold the frequent itemsets found in the last iteration\n",
    "    last_frequent_itemsets = []\n",
    "\n",
    "    for i in range(1, x+1):\n",
    "        print(f\"Generating candidates of length {i}\")\n",
    "\n",
    "        # Generate candidates of the current length\n",
    "        if (i==1):\n",
    "            candidates = generate_candidate(data, i)\n",
    "        else:\n",
    "            candidates = generate_candidate(frequent_itemsets, i)\n",
    "            #print(f\"Candidates of length {i} count: {len(candidates)}\")\n",
    "\n",
    "        # Initialize a new list to store frequent candidates\n",
    "        frequent_candidates = []\n",
    "\n",
    "        # Filter out candidates that contain infrequent subsets if i > 1\n",
    "        for candidate in candidates:\n",
    "            if has_infrequent_subset(candidate, all_frequent_itemsets) == False:\n",
    "                frequent_candidates.append(candidate)\n",
    "\n",
    "        # Calculate the support for each candidate and filter them based on the support threshold\n",
    "        frequent_itemsets = filter_candidates_by_support(data, frequent_candidates, min_support)\n",
    "        print(f\"Candidates of length {i} count: {len(frequent_candidates)}\")\n",
    "        print(\"After Pruning count:\",len(frequent_itemsets))\n",
    "        \n",
    "        \n",
    "        if len(frequent_itemsets)==0:\n",
    "            print(f\"No frequent itemsets of length {i}. Ending the algorithm.\")\n",
    "            break\n",
    "\n",
    "        #print(f\"Frequent itemsets of length {i}: {frequent_itemsets}\")\n",
    "        # Prepare for the next iteration\n",
    "        last_frequent_itemsets = frequent_itemsets\n",
    "        all_frequent_itemsets.extend(frequent_itemsets)\n",
    "        \n",
    "    #print(f\"All frequent itemsets: {all_frequent_itemsets}\")\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "x = 4\n",
    "min_support = 0.2\n",
    "# data = [[1,2,3,4,5],[4,6]]\n",
    "data = load_data(\"movie_rated.txt\")\n",
    "frequent_itemsets = apriori_algorithm(data, x, min_support)\n",
    "\n",
    "test = [[1,2,3,4,5],[4,6]]\n",
    "# print((generate_candidate(test, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print your final association rules in the following format:\n",
    "\n",
    "**movie_name_1, movie_name_2, ... --> movie_name_k**\n",
    "\n",
    "where the movie names can be fetched by joining the movieId with the file 'movies.csv'. For example, one rule that you should find is:\n",
    "\n",
    "**Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)**\n",
    "\n",
    "**Hint: if you implement the algorith correctly, you will find 14 rules in total.**\n",
    "\n",
    "**Hint: You may need to use the Pandas library to load and process the movies.csv file, such as use pandas.read_csv() to load the data. https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html is a good place to learn the basics about Pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{260, 527}, {2987, 260}, {260, 1197}, {260, 1221}, {858, 260}, {480, 260}, {260, 1127}, {1387, 260}, {260, 2028}, {1265, 260}, {260, 1270}, {377, 260}, {1197, 527}, {858, 527}, {480, 527}, {2028, 527}, {1265, 527}, {1270, 527}, {2987, 1197}, {2987, 1270}, {858, 1197}, {480, 1197}, {2028, 1197}, {1265, 1197}, {1197, 1270}, {858, 1221}, {480, 858}, {858, 2028}, {858, 1270}, {480, 2028}, {480, 1265}, {480, 1270}, {480, 377}, {1265, 2028}, {2028, 1270}, {1265, 1270}, {480, 260, 1197}, {480, 260, 2028}, {480, 260, 1270}, {858, 260, 1221}, {260, 1197, 2028}, {1265, 260, 1197}, {260, 1197, 1270}, {260, 2028, 527}, {1270, 260, 2028}, {260, 1270, 527}, {1265, 260, 1270}, {1265, 1197, 1270}]\n"
     ]
    }
   ],
   "source": [
    "# Write your code to print out the rules\n",
    "frequent_itemsets = [itemset for itemset in frequent_itemsets if len(itemset) > 1]\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('movies.csv')\n",
    "movie_name = []\n",
    "movie_id = []\n",
    "for i in range(len(movies_df)):\n",
    "    movie_name.append(movies_df.iloc[i,1])\n",
    "    movie_id.append(movies_df.iloc[i,0])\n",
    "    \n",
    "movie_dict = dict(zip(movie_id, movie_name))\n",
    "# print(movie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jaws (1975)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Godfather: Part II, The (1974)'] --> ['Godfather, The (1972)']\n",
      "['Jurassic Park (1993)', 'Princess Bride, The (1987)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Jurassic Park (1993)', 'Saving Private Ryan (1998)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Jurassic Park (1993)', 'Back to the Future (1985)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Godfather, The (1972)', 'Godfather: Part II, The (1974)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Star Wars: Episode IV - A New Hope (1977)', 'Godfather: Part II, The (1974)'] --> ['Godfather, The (1972)']\n",
      "['Saving Private Ryan (1998)', 'Princess Bride, The (1987)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Star Wars: Episode IV - A New Hope (1977)', 'Princess Bride, The (1987)'] --> ['Back to the Future (1985)']\n",
      "['Princess Bride, The (1987)', 'Back to the Future (1985)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Saving Private Ryan (1998)', 'Back to the Future (1985)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Back to the Future (1985)', \"Schindler's List (1993)\"] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      "['Groundhog Day (1993)', 'Star Wars: Episode IV - A New Hope (1977)'] --> ['Back to the Future (1985)']\n",
      "['Groundhog Day (1993)', 'Princess Bride, The (1987)'] --> ['Back to the Future (1985)']\n"
     ]
    }
   ],
   "source": [
    "def generate_rules(record,frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "         for i in range(1, len(itemset)):\n",
    "            for antecedent in combinations(itemset, i):\n",
    "                antecedent = set(antecedent)\n",
    "                consequent = itemset - antecedent\n",
    "                conf = calculate_support(record, itemset) / calculate_support(record, antecedent)\n",
    "                if conf >= min_confidence:\n",
    "                    rules.append((antecedent, consequent))           \n",
    "    return rules\n",
    "\n",
    "def rules_to_names(rules, movie_dict):\n",
    "    rules_with_names = []\n",
    "    for antecedent_ids, consequent_ids in rules:\n",
    "        antecedent_names = [movie_dict[id] for id in antecedent_ids if id in movie_dict]\n",
    "        consequent_names = [movie_dict[id] for id in consequent_ids if id in movie_dict]\n",
    "        rules_with_names.append((antecedent_names, consequent_names))\n",
    "        print(antecedent_names, \"-->\", consequent_names)\n",
    "    \n",
    "    return rules_with_names\n",
    "\n",
    "\n",
    "r = generate_rules(data, frequent_itemsets, 0.8)\n",
    "rules_with_movie_names = rules_to_names(r, movie_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
